import numpy as np
import json
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import os
import re

class TolstoySearchEngine:
    def __init__(self, embeddings_path, metadata_path, info_path, model_name):
        print("=" * 50)
        print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è TolstoySearchEngine...")
        print("=" * 50)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        self.embeddings = np.load(embeddings_path)
        self.metadata = json.load(open(metadata_path, 'r', encoding='utf-8'))
        self.info = json.load(open(info_path, 'r', encoding='utf-8'))
        self.model = SentenceTransformer(model_name)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—Å—Ç—ã –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–π
        self.works_texts = {}
        self._load_works_texts()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–∞–Ω–∫–æ–≤
        self.chunk_embeddings = None
        self.chunk_data = []
        
        self._load_chunk_embeddings()
        
        if self.chunk_embeddings is not None:
            self._create_chunk_texts()  # –°–æ–∑–¥–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –¥–ª—è —á–∞–Ω–∫–æ–≤
        
        print("=" * 50)
        print(f"‚úÖ –ü–û–ò–°–ö–û–í–´–ô –î–í–ò–ñ–û–ö –ó–ê–ì–†–£–ñ–ï–ù")
        print(f"üìö –ü—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–π: {len(self.metadata)}")
        print(f"üìñ –¢–µ–∫—Å—Ç–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {len(self.works_texts)}")
        print(f"üîç –û—Ç—Ä—ã–≤–∫–æ–≤: {len(self.chunk_data)}")
        print("=" * 50)

    def _load_works_texts(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–µ–∫—Å—Ç—ã –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–π –∏–∑ tolstoy_corpus"""
        print("üîç –ü–æ–∏—Å–∫ —Ç–µ–∫—Å—Ç–æ–≤ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–π...")
        
        possible_paths = [
            'data/tolstoy_corpus.json',  
        ]
        
        for path in possible_paths:
            if os.path.exists(path):
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª: {path}")
                try:
                    with open(path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        print(f"üìè –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
                        
                        # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–∞–∫ JSON
                        corpus_data = json.loads(content)
                        print(f"üìä –§–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö: {type(corpus_data)}")
                        
                        self._extract_texts_from_corpus(corpus_data)
                        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–π: {len(self.works_texts)}")
                        
                        if self.works_texts:
                            sample_title = list(self.works_texts.keys())[0]
                            sample_text = self.works_texts[sample_title]
                            print(f"üìñ –ü—Ä–∏–º–µ—Ä: '{sample_title}' - {len(sample_text)} —Å–∏–º–≤–æ–ª–æ–≤")
                            print(f"üìÑ –ù–∞—á–∞–ª–æ: {sample_text[:100]}...")
                        return
                        
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {path}: {e}")
                    import traceback
                    traceback.print_exc()
        
        print("‚ùå –§–∞–π–ª —Å —Ç–µ–∫—Å—Ç–∞–º–∏ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω")
        print("‚ö†Ô∏è –ë—É–¥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –∑–∞–≥–ª—É—à–∫–∏")

    def _extract_texts_from_corpus(self, corpus_data):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç—ã –∏–∑ –∫–æ—Ä–ø—É—Å–∞"""
        if isinstance(corpus_data, list):
            print(f"üìã –ó–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è —Å–ø–∏—Å–æ–∫ –∏–∑ {len(corpus_data)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
            for i, work in enumerate(corpus_data):
                if isinstance(work, dict):
                    title = work.get('title', f'–ü—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ_{i}')
                    text = work.get('text', '')
                    
                    if text and len(text.strip()) > 0:
                        self.works_texts[title] = text.strip()
                    else:
                        print(f"‚ö†Ô∏è –£ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è '{title}' –Ω–µ—Ç —Ç–µ–∫—Å—Ç–∞")
                else:
                    print(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —ç–ª–µ–º–µ–Ω—Ç–∞ {i}: {type(work)}")
        else:
            print(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –∫–æ—Ä–ø—É—Å–∞: {type(corpus_data)}")

    def _load_chunk_embeddings(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–∞–Ω–∫–æ–≤"""
        print("üîç –ü–æ–∏—Å–∫ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —á–∞–Ω–∫–æ–≤...")
        
        try:
            possible_paths = [
                'data/tolstoy_chunk_embeddings.npy',  # ‚Üê –≥–ª–∞–≤–Ω—ã–π –ø—É—Ç—å
                'tolstoy_chunk_embeddings.npy',
                'data/tolstoy_chunk_embeddings_complete.npy',  # –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π
                '../data/tolstoy_chunk_embeddings.npy'
            ]
            
            for path in possible_paths:
                if os.path.exists(path):
                    self.chunk_embeddings = np.load(path)
                    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω—ã –≤–µ–∫—Ç–æ—Ä—ã —á–∞–Ω–∫–æ–≤: {self.chunk_embeddings.shape}")
                    self._create_chunk_structure()
                    return
            
            print("‚ùå –§–∞–π–ª —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ —á–∞–Ω–∫–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —á–∞–Ω–∫–æ–≤: {e}")

    def _create_chunk_structure(self):
        """–°–æ–∑–¥–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —á–∞–Ω–∫–æ–≤"""
        chunk_mapping = self.info.get('chunk_mapping', [])
        print(f"üìã –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–ª—è {len(chunk_mapping)} —á–∞–Ω–∫–æ–≤...")
        
        for mapping in chunk_mapping:
            work_idx = mapping['work_idx']
            chunk_idx = mapping['chunk_idx']
            chunk_length = mapping['chunk_length']
            
            if work_idx < len(self.metadata):
                work = self.metadata[work_idx]
                
                self.chunk_data.append({
                    'work_title': work['title'],
                    'work_url': work['url'],
                    'work_id': work_idx,
                    'chunk_id': chunk_idx,
                    'original_length': chunk_length,
                    'word_count': chunk_length,
                    'total_chunks': work.get('num_chunks', 1),
                    'text': f"–û—Ç—Ä—ã–≤–æ–∫ {chunk_idx + 1} –∏–∑ '{work['title']}'"  # –≤—Ä–µ–º–µ–Ω–Ω–∞—è –∑–∞–≥–ª—É—à–∫–∞
                })

    def _create_chunk_texts(self):
        """–°–æ–∑–¥–∞–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –¥–ª—è —á–∞–Ω–∫–æ–≤ –∏–∑ –ø–æ–ª–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–π"""
        print("‚úÇÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ –æ—Ç—Ä—ã–≤–∫–æ–≤ –∏–∑ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–π...")
        
        texts_created = 0
        texts_missing = 0
        
        for i, chunk in enumerate(self.chunk_data):
            work_title = chunk['work_title']
            
            if work_title in self.works_texts:
                full_text = self.works_texts[work_title]
                chunk_text = self._extract_chunk_text(full_text, chunk)
                if chunk_text:
                    chunk['text'] = chunk_text
                    texts_created += 1
                    
                    # –í—ã–≤–æ–¥–∏–º –ø–µ—Ä–≤—ã–π —Å–æ–∑–¥–∞–Ω–Ω—ã–π –æ—Ç—Ä—ã–≤–æ–∫ –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
                    if texts_created == 1:
                        print(f"üìñ –ü—Ä–∏–º–µ—Ä –æ—Ç—Ä—ã–≤–∫–∞: {chunk_text[:100]}...")
            else:
                texts_missing += 1
                # –û—Å—Ç–∞–≤–ª—è–µ–º –∑–∞–≥–ª—É—à–∫—É, –Ω–æ –±–æ–ª–µ–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—É—é
                chunk['text'] = f"–û—Ç—Ä—ã–≤–æ–∫ –∏–∑ '{work_title}'"
        
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤ –æ—Ç—Ä—ã–≤–∫–æ–≤: {texts_created}")
        print(f"‚ö†Ô∏è –¢–µ–∫—Å—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ: {texts_missing}")

    def _extract_chunk_text(self, full_text, chunk_info):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –æ—Ç—Ä—ã–≤–æ–∫ –∏–∑ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è"""
        if not full_text or len(full_text.strip()) == 0:
            return None
            
        words = full_text.split()
        total_words = len(words)
        
        if total_words == 0:
            return None
        
        # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ–∑–∏—Ü–∏—é —á–∞–Ω–∫–∞ –≤ —Ç–µ–∫—Å—Ç–µ
        chunk_ratio = chunk_info['chunk_id'] / max(chunk_info['total_chunks'], 1)
        chunk_size = 250  # —Ü–µ–ª–µ–≤–æ–π —Ä–∞–∑–º–µ—Ä –æ—Ç—Ä—ã–≤–∫–∞ –≤ —Å–ª–æ–≤–∞—Ö
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ–∑–∏—Ü–∏—é –æ—Ç—Ä—ã–≤–∫–∞
        start_pos = int(total_words * chunk_ratio)
        start = max(0, start_pos - chunk_size // 2)
        end = min(total_words, start + chunk_size)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ç—Ä—ã–≤–æ–∫
        passage_words = words[start:end]
        
        if not passage_words:
            return None
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤ —Ç–µ–∫—Å—Ç
        passage = ' '.join(passage_words)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–Ω–æ–≥–æ—Ç–æ—á–∏—è –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ –Ω–∞—á–∞–ª–æ/–∫–æ–Ω–µ—Ü
        if start > 0:
            passage = '...' + passage
        if end < total_words:
            passage = passage + '...'
        
        # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ —Ç–µ–∫—Å—Ç –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –Ω–∞ –∑–∞–∫–æ–Ω—á–µ–Ω–Ω–æ–π –º—ã—Å–ª–∏
        passage = self._clean_passage_end(passage)
        
        return passage

    def _clean_passage_end(self, text):
        """–û—á–∏—â–∞–µ—Ç –∫–æ–Ω–µ—Ü –æ—Ç—Ä—ã–≤–∫–∞, —á—Ç–æ–±—ã –æ–Ω –∑–∞–∫–∞–Ω—á–∏–≤–∞–ª—Å—è –Ω–∞ –∑–∞–∫–æ–Ω—á–µ–Ω–Ω–æ–π –º—ã—Å–ª–∏"""
        # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Ç–æ—á–∫—É, –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π –∏–ª–∏ –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –∑–Ω–∞–∫
        for end_char in ['.', '!', '?', ';']:
            last_pos = text.rfind(end_char)
            if last_pos != -1 and last_pos > len(text) * 0.7:  # –ß—Ç–æ–±—ã –Ω–µ –æ–±—Ä–µ–∑–∞—Ç—å —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ
                return text[:last_pos + 1]
        
        return text

    def search_passages(self, query, top_k=15, min_similarity=0.3):
        """–ü–æ–∏—Å–∫ –ø–æ –æ—Ç—Ä—ã–≤–∫–∞–º —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏"""
        print(f"üîç –ü–æ–∏—Å–∫ –æ—Ç—Ä—ã–≤–∫–æ–≤ –¥–ª—è: '{query}'")
        
        if self.chunk_embeddings is None or len(self.chunk_data) == 0:
            print("‚ö†Ô∏è –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–∞–Ω–∫–æ–≤ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–∏—Å–∫ –ø–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è–º")
            return self._fallback_search_works(query, top_k)
        
        # –ö–æ–¥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å
        query_vector = self.model.encode([query])
        query_vector = query_vector / np.linalg.norm(query_vector, axis=1, keepdims=True)
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å —Å —á–∞–Ω–∫–∞–º–∏
        similarities = cosine_similarity(query_vector, self.chunk_embeddings)[0]
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø-K —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        
        results = []
        for i, idx in enumerate(top_indices):
            if idx >= len(self.chunk_data):
                continue
                
            similarity = similarities[idx]
            
            if similarity < min_similarity:
                continue
                
            chunk_info = self.chunk_data[idx]
            
            results.append({
                'rank': i + 1,
                'work_title': chunk_info['work_title'],
                'work_url': chunk_info['work_url'],
                'work_id': chunk_info['work_id'],
                'passage': chunk_info['text'],
                'similarity': float(similarity),
                'similarity_percent': round(float(similarity) * 100, 1),
                'word_count': chunk_info['word_count'],
                'passage_length': chunk_info['word_count']
            })
        
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –æ—Ç—Ä—ã–≤–∫–æ–≤: {len(results)}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –æ—Ç–ª–∞–¥–æ—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–µ—Ä–≤–æ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ
        if results:
            first_result = results[0]
            print(f"üìñ –ü–µ—Ä–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: '{first_result['work_title']}'")
            print(f"üìÑ –¢–µ–∫—Å—Ç: {first_result['passage'][:100]}...")
        
        return results

    # –û—Å—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π...
    def _fallback_search_works(self, query, top_k=15):
        works_results = self.search_works(query, top_k)
        
        passages_results = []
        for work in works_results:
            work_text = self.works_texts.get(work['title'], '')
            if work_text:
                preview = work_text[:300] + "..." if len(work_text) > 300 else work_text
                passage_text = preview
            else:
                passage_text = f"üìñ {work['title']}"
            
            passages_results.append({
                'rank': work['rank'],
                'work_title': work['title'],
                'work_url': work['url'],
                'work_id': self._get_work_id_by_title(work['title']),
                'passage': passage_text,
                'similarity': work['similarity'],
                'similarity_percent': work['similarity_percent'],
                'word_count': work['original_length'],
                'passage_length': work['original_length']
            })
        
        return passages_results

    def _get_work_id_by_title(self, title):
        for work in self.metadata:
            if work['title'] == title:
                return work['id']
        return 0

    def search_works(self, query, top_k=20):
        query_vector = self.model.encode([query])
        query_vector = query_vector / np.linalg.norm(query_vector, axis=1, keepdims=True)
        
        similarities = cosine_similarity(query_vector, self.embeddings)[0]
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        
        results = []
        for i, idx in enumerate(top_indices):
            work = self.metadata[idx]
            results.append({
                'rank': i + 1,
                'title': work['title'],
                'url': work['url'],
                'similarity': float(similarities[idx]),
                'similarity_percent': round(float(similarities[idx]) * 100, 1),
                'original_length': work['original_length']
            })
        
        return results

    def find_similar_works(self, work_id, top_k=5):
        work_vector = self.embeddings[work_id:work_id+1]
        similarities = cosine_similarity(work_vector, self.embeddings)[0]
        
        top_indices = np.argsort(similarities)[-top_k-1:][::-1]
        
        similar = []
        for idx in top_indices:
            if idx != work_id and len(similar) < top_k:
                work = self.metadata[idx]
                similar.append({
                    'title': work['title'],
                    'url': work['url'],
                    'similarity': float(similarities[idx]),
                    'similarity_percent': round(float(similarities[idx]) * 100, 1)
                })
        
        return similar
    
    def get_work_by_id(self, work_id):
        return self.metadata[work_id]
    
    def get_all_works(self):
        return self.metadata
