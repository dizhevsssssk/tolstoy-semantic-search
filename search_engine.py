import numpy as np
import json
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import os
import re

class TolstoySearchEngine:
    def __init__(self, embeddings_path, metadata_path, info_path, model_name, verbose=False):
        self.verbose = verbose
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        self.embeddings = np.load(embeddings_path)
        self.metadata = json.load(open(metadata_path, 'r', encoding='utf-8'))
        self.info = json.load(open(info_path, 'r', encoding='utf-8'))
        self.model = SentenceTransformer(model_name)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—Å—Ç—ã –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–π —Å –ø—É–Ω–∫—Ç—É–∞—Ü–∏–µ–π
        self.works_texts = {}
        self._load_works_texts_with_punctuation()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–∞–Ω–∫–æ–≤
        self.chunk_embeddings = None
        self.chunk_data = []
        self._load_chunk_embeddings()
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç—ã –¥–ª—è —á–∞–Ω–∫–æ–≤
        if self.chunk_embeddings is not None and self.works_texts:
            self._create_chunk_texts_from_originals()
        
        if self.verbose:
            print(f"–ü–æ–∏—Å–∫–æ–≤—ã–π –¥–≤–∏–∂–æ–∫ –∑–∞–≥—Ä—É–∂–µ–Ω: {len(self.metadata)} –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–π, {len(self.chunk_data)} –æ—Ç—Ä—ã–≤–∫–æ–≤")

    def _load_works_texts_with_punctuation(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–µ–∫—Å—Ç—ã –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–π —Å –ø—É–Ω–∫—Ç—É–∞—Ü–∏–µ–π"""
        path = 'data/tolstoy_corpus_with_punctuation.json'
        if not os.path.exists(path):
            return
        
        try:
            with open(path, 'r', encoding='utf-8') as f:
                corpus_data = json.load(f)
                if isinstance(corpus_data, list):
                    for work in corpus_data:
                        if isinstance(work, dict):
                            title = work.get('title', '')
                            text = work.get('text', '')
                            if title and text and len(text.strip()) > 0:
                                self.works_texts[title] = text.strip()
        except Exception as e:
            if self.verbose:
                print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤: {e}")

    def _load_chunk_embeddings(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–∞–Ω–∫–æ–≤"""
        path = 'data/tolstoy_chunk_embeddings.npy'
        if not os.path.exists(path):
            return
        
        try:
            self.chunk_embeddings = np.load(path)
            self._create_chunk_structure()
        except Exception as e:
            if self.verbose:
                print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —á–∞–Ω–∫–æ–≤: {e}")

    def _create_chunk_structure(self):
        """–°–æ–∑–¥–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —á–∞–Ω–∫–æ–≤"""
        chunk_mapping = self.info.get('chunk_mapping', [])
        for mapping in chunk_mapping:
            work_idx = mapping['work_idx']
            if work_idx < len(self.metadata):
                work = self.metadata[work_idx]
                self.chunk_data.append({
                    'work_title': work['title'],
                    'work_url': work['url'],
                    'work_id': work_idx,
                    'chunk_id': mapping['chunk_idx'],
                    'original_length': mapping['chunk_length'],
                    'word_count': mapping['chunk_length'],
                    'total_chunks': work.get('num_chunks', 1),
                    'text': f"–û—Ç—Ä—ã–≤–æ–∫ {mapping['chunk_idx'] + 1} –∏–∑ '{work['title']}'"
                })

    def _clean_text(self, text, work_title):
        """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
        if not text:
            return text
        
        lines = text.split('\n')
        cleaned_lines = []
        in_content = False
        
        for line in lines:
            line = line.strip()
            if not line and not in_content:
                continue
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
            if (len(line) < 50 and line.isupper()) or line == work_title or line == work_title.upper():
                continue
            
            if not in_content:
                in_content = True
            cleaned_lines.append(line)
        
        cleaned_text = ' '.join(cleaned_lines)
        cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
        return cleaned_text

    def _extract_chunk(self, full_text, chunk_info):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Ç—Ä—ã–≤–æ–∫ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        if not full_text:
            return None
        
        work_title = chunk_info['work_title']
        clean_text = self._clean_text(full_text, work_title)
        
        if len(clean_text) < 500:
            return clean_text[:500] + "..." if len(clean_text) > 500 else clean_text
        
        total_chars = len(clean_text)
        chunk_ratio = chunk_info['chunk_id'] / max(chunk_info['total_chunks'], 1)
        chunk_size = 1000
        
        start_pos = int(total_chars * chunk_ratio)
        start = max(0, start_pos - 100)
        end = min(total_chars, start + chunk_size)
        
        # –ò—â–µ–º –Ω–∞—á–∞–ª–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        for i in range(start, max(0, start - 200), -1):
            if i > 0 and clean_text[i-1] in '.!?':
                start = i
                break
        
        # –ò—â–µ–º –∫–æ–Ω–µ—Ü –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        for i in range(end, min(total_chars, end + 200)):
            if i < total_chars and clean_text[i] in '.!?':
                end = i + 1
                break
        
        passage = clean_text[start:end]
        if start > 0:
            passage = '...' + passage
        if end < total_chars:
            passage = passage + '...'
        
        return passage

    def _create_chunk_texts_from_originals(self):
        """–°–æ–∑–¥–∞–µ—Ç —Ç–µ–∫—Å—Ç—ã –¥–ª—è —á–∞–Ω–∫–æ–≤ –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤"""
        for chunk in self.chunk_data:
            work_title = chunk['work_title']
            if work_title in self.works_texts:
                full_text = self.works_texts[work_title]
                chunk_text = self._extract_chunk(full_text, chunk)
                if chunk_text:
                    chunk['text'] = chunk_text

    def _split_into_sentences(self, text):
        """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"""
        if not text:
            return []
        # –ü—Ä–æ—Å—Ç–∞—è —Ä–∞–∑–±–∏–≤–∫–∞ –ø–æ —Ç–æ—á–∫–∞–º, –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º –∏ –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–º –∑–Ω–∞–∫–∞–º
        # –£—á–∏—Ç—ã–≤–∞–µ–º –º–Ω–æ–≥–æ—Ç–æ—á–∏—è –∏ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è
        import re
        # –†–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è —Ä–∞–∑–±–∏–≤–∫–∏ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentence_endings = r'(?<=[.!?])\s+(?=[–ê-–ØA-Z])'
        sentences = re.split(sentence_endings, text)
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—ã–µ
        sentences = [s.strip() for s in sentences if s.strip()]
        return sentences

    def _find_most_relevant_sentence(self, passage, query):
        """–ù–∞—Ö–æ–¥–∏—Ç –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –≤ –æ—Ç—Ä—ã–≤–∫–µ –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"""
        sentences = self._split_into_sentences(passage)
        if not sentences:
            return passage, 0, 0.0  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤–µ—Å—å –æ—Ç—Ä—ã–≤–æ–∫, –∏–Ω–¥–µ–∫—Å 0, —Å—Ö–æ–∂–µ—Å—Ç—å 0
        
        # –ö–æ–¥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –æ–¥–∏–Ω —Ä–∞–∑
        query_vector = self.model.encode([query])
        query_vector = query_vector / np.linalg.norm(query_vector, axis=1, keepdims=True)
        
        # –ö–æ–¥–∏—Ä—É–µ–º –≤—Å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentence_vectors = self.model.encode(sentences)
        sentence_vectors = sentence_vectors / np.linalg.norm(sentence_vectors, axis=1, keepdims=True)
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å
        similarities = cosine_similarity(query_vector, sentence_vectors)[0]
        best_idx = np.argmax(similarities)
        best_similarity = similarities[best_idx]
        
        # –ï—Å–ª–∏ —Å—Ö–æ–∂–µ—Å—Ç—å –æ—á–µ–Ω—å –Ω–∏–∑–∫–∞—è (–º–µ–Ω—å—à–µ 0.1), –Ω–µ –≤—ã–¥–µ–ª—è–µ–º –Ω–∏—á–µ–≥–æ
        if best_similarity < 0.1:
            return passage, -1, best_similarity
        
        return sentences[best_idx], best_idx, best_similarity

    def _highlight_most_relevant_sentence(self, passage, query):
        """–í—ã–¥–µ–ª—è–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –≤ –æ—Ç—Ä—ã–≤–∫–µ HTML-—Ç–µ–≥–∞–º–∏"""
        sentences = self._split_into_sentences(passage)
        if not sentences:
            return passage, False
        
        best_sentence, best_idx, similarity = self._find_most_relevant_sentence(passage, query)
        if best_idx == -1:
            return passage, False
        
        # –ó–∞–º–µ–Ω—è–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –Ω–∞ –≤–µ—Ä—Å–∏—é —Å —Ç–µ–≥–æ–º
        sentences[best_idx] = f'<span class="highlight-sentence">{sentences[best_idx]}</span>'
        highlighted_passage = ' '.join(sentences)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ç–µ–≥ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω
        if '<span class="highlight-sentence">' in highlighted_passage:
            return highlighted_passage, True
        else:
            return passage, False

    def search_passages(self, query, top_k=15, min_similarity=0.3):
        """–ü–æ–∏—Å–∫ –ø–æ –æ—Ç—Ä—ã–≤–∫–∞–º"""
        if self.chunk_embeddings is None or len(self.chunk_data) == 0:
            return self._fallback_search_works(query, top_k)
        
        # –ö–æ–¥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å
        query_vector = self.model.encode([query])
        query_vector = query_vector / np.linalg.norm(query_vector, axis=1, keepdims=True)
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å
        similarities = cosine_similarity(query_vector, self.chunk_embeddings)[0]
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø-K —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        top_indices = np.argsort(similarities)[-top_k*2:][::-1]
        
        results = []
        seen_texts = set()
        
        for idx in top_indices:
            if idx >= len(self.chunk_data):
                continue
                
            similarity = similarities[idx]
            if similarity < min_similarity:
                continue
                
            chunk_info = self.chunk_data[idx]
            passage_text = chunk_info['text']
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å
            text_fingerprint = passage_text[:100].lower()
            if text_fingerprint in seen_texts:
                continue
            
            seen_texts.add(text_fingerprint)
            
            # –í—ã–¥–µ–ª—è–µ–º –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
            highlighted_passage, has_highlight = self._highlight_most_relevant_sentence(passage_text, query)
            
            results.append({
                'rank': len(results) + 1,
                'work_title': chunk_info['work_title'],
                'work_url': chunk_info['work_url'],
                'work_id': chunk_info['work_id'],
                'passage': highlighted_passage,
                'similarity': float(similarity),
                'similarity_percent': round(float(similarity) * 100, 1),
                'word_count': chunk_info['word_count'],
                'passage_length': chunk_info['word_count'],
                'has_highlight': has_highlight
            })
            
            if len(results) >= top_k:
                break
        
        return results

    def _fallback_search_works(self, query, top_k=15):
        """–§–æ–ª–±—ç–∫ –ø–æ–∏—Å–∫ –ø–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è–º"""
        works_results = self.search_works(query, top_k)
        passages_results = []
        
        for work in works_results:
            work_text = self.works_texts.get(work['title'], '')
            if work_text:
                preview = work_text[:300] + "..." if len(work_text) > 300 else work_text
                passage_text = preview
            else:
                passage_text = f"üìñ {work['title']}"
            
            passages_results.append({
                'rank': work['rank'],
                'work_title': work['title'],
                'work_url': work['url'],
                'work_id': self._get_work_id_by_title(work['title']),
                'passage': passage_text,
                'similarity': work['similarity'],
                'similarity_percent': work['similarity_percent'],
                'word_count': work['original_length'],
                'passage_length': work['original_length']
            })
        
        return passages_results

    def _get_work_id_by_title(self, title):
        """–ù–∞—Ö–æ–¥–∏—Ç ID –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é"""
        for i, work in enumerate(self.metadata):
            if work['title'] == title:
                return i
        return 0

    def search_works(self, query, top_k=20):
        """–ü–æ–∏—Å–∫ –ø–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è–º"""
        query_vector = self.model.encode([query])
        query_vector = query_vector / np.linalg.norm(query_vector, axis=1, keepdims=True)
        
        similarities = cosine_similarity(query_vector, self.embeddings)[0]
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        
        results = []
        for i, idx in enumerate(top_indices):
            work = self.metadata[idx]
            results.append({
                'rank': i + 1,
                'title': work['title'],
                'url': work['url'],
                'similarity': float(similarities[idx]),
                'similarity_percent': round(float(similarities[idx]) * 100, 1),
                'original_length': work['original_length']
            })
        
        return results

    def find_similar_works(self, work_id, top_k=5):
        """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–π"""
        work_vector = self.embeddings[work_id:work_id+1]
        similarities = cosine_similarity(work_vector, self.embeddings)[0]
        
        top_indices = np.argsort(similarities)[-top_k-1:][::-1]
        
        similar = []
        for idx in top_indices:
            if idx != work_id and len(similar) < top_k:
                work = self.metadata[idx]
                similar.append({
                    'title': work['title'],
                    'url': work['url'],
                    'similarity': float(similarities[idx]),
                    'similarity_percent': round(float(similarities[idx]) * 100, 1)
                })
        
        return similar
    
    def get_work_by_id(self, work_id):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –ø–æ ID"""
        return self.metadata[work_id]
    
    def get_all_works(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–π"""
        return self.metadata
